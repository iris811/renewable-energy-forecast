# í•™ìŠµ ê°€ì´ë“œ

ì¬ìƒì—ë„ˆì§€ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
3. [í•™ìŠµ í”„ë¡œì„¸ìŠ¤](#í•™ìŠµ-í”„ë¡œì„¸ìŠ¤)
4. [Loss Functions](#loss-functions)
5. [Metrics](#metrics)
6. [ê³ ê¸‰ ì„¤ì •](#ê³ ê¸‰-ì„¤ì •)

---

## ê°œìš”

ì´ ì‹œìŠ¤í…œì€ ì¬ìƒì—ë„ˆì§€ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤:

- âœ… ìë™ ë°ì´í„° ì „ì²˜ë¦¬
- âœ… ë‹¤ì–‘í•œ Loss functions
- âœ… Early stopping & Learning rate scheduling
- âœ… ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
- âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- âœ… í‰ê°€ ì§€í‘œ ê³„ì‚°

---

## ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ í•™ìŠµ

```bash
python scripts/train_model.py
```

ê¸°ë³¸ ì„¤ì •:
- Model: LSTM
- Location: Seoul Solar Farm
- Energy Type: Solar
- Epochs: 100
- Batch Size: 32

### 2. ì˜µì…˜ ì§€ì •

```bash
python scripts/train_model.py \
  --model-type transformer \
  --location "Seoul Solar Farm" \
  --energy-type solar \
  --epochs 150 \
  --batch-size 32 \
  --lr 0.0001 \
  --gpu 0
```

**ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜:**
- `--model-type`: lstm, lstm_attention, transformer, timeseries
- `--location`: ìœ„ì¹˜ ì´ë¦„ (config.yamlì— ì •ì˜)
- `--energy-type`: solar, wind
- `--epochs`: í•™ìŠµ ì—í­ ìˆ˜
- `--batch-size`: ë°°ì¹˜ í¬ê¸°
- `--lr`: í•™ìŠµë¥ 
- `--gpu`: GPU ID (Noneì´ë©´ ìë™ ì„ íƒ)
- `--resume`: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ

### 3. í•™ìŠµ ì¬ê°œ

```bash
python scripts/train_model.py --resume models/checkpoints/last_model.pth
```

---

## í•™ìŠµ í”„ë¡œì„¸ìŠ¤

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```
1. ë°ì´í„° ì¤€ë¹„ (DataPipeline)
   â”œâ”€â”€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
   â”œâ”€â”€ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ìƒì„±
   â”œâ”€â”€ ìŠ¤ì¼€ì¼ë§
   â””â”€â”€ ì‹œí€€ìŠ¤ ìƒì„±

2. ëª¨ë¸ ìƒì„± (create_model)
   â””â”€â”€ ì§€ì •ëœ íƒ€ì…ì˜ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

3. Trainer ì´ˆê¸°í™”
   â”œâ”€â”€ Loss function ì„¤ì •
   â”œâ”€â”€ Optimizer ì„¤ì •
   â”œâ”€â”€ Early stopping ì„¤ì •
   â””â”€â”€ Learning rate scheduler ì„¤ì •

4. í•™ìŠµ (Trainer.train)
   â”œâ”€â”€ Epoch ë£¨í”„
   â”‚   â”œâ”€â”€ í•™ìŠµ (train_epoch)
   â”‚   â”œâ”€â”€ ê²€ì¦ (validate)
   â”‚   â”œâ”€â”€ Metrics ê³„ì‚°
   â”‚   â”œâ”€â”€ Learning rate ì¡°ì •
   â”‚   â”œâ”€â”€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
   â”‚   â””â”€â”€ Early stopping ì²´í¬
   â””â”€â”€ í•™ìŠµ ì™„ë£Œ

5. í‰ê°€ (Trainer.evaluate)
   â””â”€â”€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ìµœì¢… í‰ê°€
```

### Python ì½”ë“œë¡œ í•™ìŠµ

```python
from src.preprocessing import DataPipeline
from src.models import create_model
from src.training import Trainer, get_loss_function
from src.models import get_device

# 1. ë°ì´í„° ì¤€ë¹„
pipeline = DataPipeline(
    location_name='Seoul Solar Farm',
    energy_type='solar',
    latitude=37.5665,
    longitude=126.9780,
    sequence_length=168,
    prediction_horizon=24
)

result = pipeline.run_pipeline(batch_size=32)
train_loader = result['train_loader']
val_loader = result['val_loader']
test_loader = result['test_loader']

# 2. ëª¨ë¸ ìƒì„±
model = create_model(
    model_type='lstm',
    input_dim=result['n_features'],
    output_dim=24,
    sequence_length=168
)

# 3. Trainer ì„¤ì •
device = get_device()
criterion = get_loss_function('rmse')
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

config = {
    'early_stopping': {'enabled': True, 'patience': 10},
    'scheduler': {'type': 'ReduceLROnPlateau', 'patience': 5},
    'checkpoint': {'save_best': True, 'save_dir': 'models/checkpoints'}
}

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    device=device,
    config=config
)

# 4. í•™ìŠµ
history = trainer.train(epochs=100)

# 5. í‰ê°€
metrics = trainer.evaluate(test_loader)
```

---

## Loss Functions

### ì‚¬ìš© ê°€ëŠ¥í•œ Loss Functions

#### 1. MSE (Mean Squared Error)
```python
criterion = get_loss_function('mse')
```
- ê°€ì¥ ê¸°ë³¸ì ì¸ íšŒê·€ ì†ì‹¤
- í° ì˜¤ì°¨ì— ë” í° í˜ë„í‹°

#### 2. MAE (Mean Absolute Error)
```python
criterion = get_loss_function('mae')
```
- ì´ìƒì¹˜ì— ëœ ë¯¼ê°
- ëª¨ë“  ì˜¤ì°¨ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜

#### 3. RMSE (Root Mean Squared Error)
```python
criterion = get_loss_function('rmse')
```
- MSEì˜ ì œê³±ê·¼
- ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ í•´ì„ ê°€ëŠ¥

#### 4. Huber Loss
```python
criterion = get_loss_function('huber', delta=1.0)
```
- MSEì™€ MAEì˜ ê²°í•©
- ì´ìƒì¹˜ì— ê°•ê±´

#### 5. MAPE (Mean Absolute Percentage Error)
```python
criterion = get_loss_function('mape')
```
- ë°±ë¶„ìœ¨ ì˜¤ì°¨
- ìŠ¤ì¼€ì¼ ë…ë¦½ì 

#### 6. Weighted MSE
```python
criterion = get_loss_function('weighted_mse')
```
- ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ ì ìš©
- ë‹¨ê¸° ì˜ˆì¸¡ì— ë” í° ê°€ì¤‘ì¹˜

#### 7. Asymmetric Loss
```python
criterion = get_loss_function('asymmetric', beta=2.0)
```
- ê³¼ì†Œ ì˜ˆì¸¡ì— ë” í° í˜ë„í‹°
- ì¬ìƒì—ë„ˆì§€ ì˜ˆì¸¡ì— ìœ ìš©

### Loss Function ë¹„êµ

| Loss | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|------|------|------|---------------|
| MSE | ìˆ˜í•™ì ìœ¼ë¡œ ê°„ë‹¨ | ì´ìƒì¹˜ì— ë¯¼ê° | ì¼ë°˜ì ì¸ íšŒê·€ |
| MAE | ì´ìƒì¹˜ì— ê°•ê±´ | ìµœì í™” ëŠë¦¼ | ì´ìƒì¹˜ ë§ì€ ë°ì´í„° |
| RMSE | í•´ì„ ìš©ì´ | MSEì™€ ìœ ì‚¬ | í‘œì¤€ í‰ê°€ |
| Huber | ê°•ê±´ì„± | í•˜ì´í¼íŒŒë¼ë¯¸í„° í•„ìš” | ì´ìƒì¹˜ ì¡´ì¬ |
| MAPE | ìŠ¤ì¼€ì¼ ë…ë¦½ | 0ì— ê°€ê¹Œìš´ ê°’ ë¬¸ì œ | ë¹„ìœ¨ ì¤‘ìš” |
| Weighted MSE | ì‹œê°„ëŒ€ë³„ ì°¨ë³„í™” | ê°€ì¤‘ì¹˜ ì„¤ì • í•„ìš” | ë‹¨ê¸° ì˜ˆì¸¡ ì¤‘ìš” |
| Asymmetric | ë¹„ëŒ€ì¹­ í˜ë„í‹° | ë¶ˆê· í˜• ë¬¸ì œ | ê³¼ì†Œì˜ˆì¸¡ ìœ„í—˜ |

---

## Metrics

### í‰ê°€ ì§€í‘œ

#### 1. RMSE (Root Mean Squared Error)
```
RMSE = sqrt(mean((y_pred - y_true)^2))
```
- **ì˜ë¯¸**: ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ì˜ í‰ê·  í¸ì°¨ (ì œê³±ê·¼)
- **ë‹¨ìœ„**: ì›ë˜ ë°ì´í„°ì™€ ë™ì¼ (kW)
- **ì¢‹ì€ ê°’**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ

#### 2. MAE (Mean Absolute Error)
```
MAE = mean(abs(y_pred - y_true))
```
- **ì˜ë¯¸**: ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· 
- **ë‹¨ìœ„**: ì›ë˜ ë°ì´í„°ì™€ ë™ì¼ (kW)
- **ì¢‹ì€ ê°’**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ

#### 3. MAPE (Mean Absolute Percentage Error)
```
MAPE = mean(abs((y_true - y_pred) / y_true)) * 100
```
- **ì˜ë¯¸**: ë°±ë¶„ìœ¨ ì˜¤ì°¨ì˜ í‰ê· 
- **ë‹¨ìœ„**: %
- **ì¢‹ì€ ê°’**: <10% (ìš°ìˆ˜), 10-20% (ì¢‹ìŒ), >20% (ê°œì„  í•„ìš”)

#### 4. RÂ² (R-squared)
```
RÂ² = 1 - (SS_res / SS_tot)
```
- **ì˜ë¯¸**: ëª¨ë¸ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì˜ ë¹„ìœ¨
- **ë²”ìœ„**: -âˆ ~ 1
- **ì¢‹ì€ ê°’**: 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ (1 = ì™„ë²½í•œ ì˜ˆì¸¡)

#### 5. NRMSE (Normalized RMSE)
```
NRMSE = RMSE / (max(y_true) - min(y_true)) * 100
```
- **ì˜ë¯¸**: ë°ì´í„° ë²”ìœ„ë¡œ ì •ê·œí™”ëœ RMSE
- **ë‹¨ìœ„**: %
- **ì¢‹ì€ ê°’**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ

#### 6. MBE (Mean Bias Error)
```
MBE = mean(y_pred - y_true)
```
- **ì˜ë¯¸**: ì²´ê³„ì  ê³¼ëŒ€/ê³¼ì†Œ ì˜ˆì¸¡
- **ì–‘ìˆ˜**: ê³¼ëŒ€ ì˜ˆì¸¡
- **ìŒìˆ˜**: ê³¼ì†Œ ì˜ˆì¸¡
- **0**: í¸í–¥ ì—†ìŒ

### Metrics ê³„ì‚°

```python
from src.training import MetricsCalculator

calculator = MetricsCalculator()

# ëª¨ë“  ì§€í‘œ ê³„ì‚°
metrics = calculator.calculate_all(y_true, y_pred)

# ì‹œê°„ëŒ€ë³„ ì§€í‘œ
per_horizon_metrics = calculator.calculate_per_horizon(
    y_true, y_pred,
    horizon_names=['1h', '2h', ..., '24h']
)

# ì¶œë ¥
calculator.print_metrics(metrics)
```

---

## ê³ ê¸‰ ì„¤ì •

### Early Stopping

```yaml
# configs/config.yaml
training:
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.0001
```

- **patience**: ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦´ ì—í­ ìˆ˜
- **min_delta**: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰

### Learning Rate Scheduler

```yaml
training:
  scheduler:
    type: "ReduceLROnPlateau"
    patience: 5
    factor: 0.5
    min_lr: 0.00001
```

- **patience**: LR ê°ì†Œ ì „ ê¸°ë‹¤ë¦´ ì—í­
- **factor**: LR ê°ì†Œ ë¹„ìœ¨
- **min_lr**: ìµœì†Œ í•™ìŠµë¥ 

### Gradient Clipping

```yaml
training:
  grad_clip: 1.0  # 0ì´ë©´ ë¹„í™œì„±í™”
```

### ì²´í¬í¬ì¸íŠ¸

```yaml
training:
  checkpoint:
    save_best: true
    save_last: true
    save_dir: "./models/checkpoints"
```

ì €ì¥ë˜ëŠ” íŒŒì¼:
- `best_model.pth`: ê²€ì¦ ì†ì‹¤ì´ ê°€ì¥ ë‚®ì€ ëª¨ë¸
- `last_model.pth`: ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸

### ì»¤ìŠ¤í…€ í•™ìŠµ ë£¨í”„

```python
from src.training import Trainer

class CustomTrainer(Trainer):
    def train_epoch(self):
        # ì»¤ìŠ¤í…€ í•™ìŠµ ë¡œì§
        pass

    def validate(self):
        # ì»¤ìŠ¤í…€ ê²€ì¦ ë¡œì§
        pass
```

---

## í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ë¡œê·¸

í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥:

```
Epoch 10/100 | Train RMSE: 0.0542 | Val RMSE: 0.0618 | LR: 0.001000 | Time: 23.45s
EarlyStopping counter: 3/10
Validation score improved by 0.001234
Saved best model (Val RMSE: 0.0618)
```

### í•™ìŠµ íˆìŠ¤í† ë¦¬

```python
history = trainer.train(epochs=100)

# ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„°
train_losses = history['train_loss']
val_losses = history['val_loss']
learning_rates = history['learning_rate']
epoch_times = history['epoch_time']

# ì‹œê°í™”
import matplotlib.pyplot as plt

plt.plot(train_losses, label='Train')
plt.plot(val_losses, label='Validation')
plt.legend()
plt.show()
```

---

## ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```
RuntimeError: CUDA out of memory
```
**í•´ê²°ì±…:**
- Batch size ì¤„ì´ê¸°: `--batch-size 16`
- Sequence length ì¤„ì´ê¸°: config.yamlì—ì„œ ìˆ˜ì •
- ì‘ì€ ëª¨ë¸ ì‚¬ìš©: `--model-type lstm` (ëŒ€ì‹  transformer)

### í•™ìŠµì´ ëŠë¦¼
**í•´ê²°ì±…:**
- GPU ì‚¬ìš©: `--gpu 0`
- Batch size ì¦ê°€: `--batch-size 64`
- ì‘ì€ ëª¨ë¸ ì‚¬ìš©

### Overfitting
**ì¦ìƒ:** Train lossëŠ” ê°ì†Œ, Val lossëŠ” ì¦ê°€

**í•´ê²°ì±…:**
- Early stopping patience ì¤„ì´ê¸°
- Dropout ì¦ê°€: config.yamlì—ì„œ `dropout: 0.3`
- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
- ì •ê·œí™”: `weight_decay` ì¦ê°€

### Underfitting
**ì¦ìƒ:** Train/Val loss ëª¨ë‘ ë†’ìŒ

**í•´ê²°ì±…:**
- ë” í° ëª¨ë¸ ì‚¬ìš©: `--model-type transformer`
- ë” ê¸´ í•™ìŠµ: `--epochs 200`
- Learning rate ì¡°ì •: `--lr 0.01`

---

## ë‹¤ìŒ ë‹¨ê³„

í•™ìŠµì´ ì™„ë£Œë˜ë©´:

1. **ëª¨ë¸ í‰ê°€**: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ìƒì„¸ í‰ê°€
2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ìµœì  ì„¤ì • ì°¾ê¸°
3. **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ ê²°í•©
4. **ë°°í¬**: APIë¡œ ì„œë¹™

ê´€ë ¨ ê°€ì´ë“œ:
- ëª¨ë¸ ì„ íƒ: `MODEL_GUIDE.md`
- ì „ì²˜ë¦¬: `DATA_PREPROCESSING_GUIDE.md`
- ë°°í¬: `DEPLOYMENT_GUIDE.md`
